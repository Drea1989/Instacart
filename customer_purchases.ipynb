{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import the needed libraries\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') #Supress unnecessary warnings for readability and cleaner presentation\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "color = sns.color_palette()\n",
    "\n",
    "import pickle\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x) #Limiting floats output to 3 decimal points\n",
    "import gc\n",
    "print (gc.isenabled())\n",
    "\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Model,Sequential\n",
    "\n",
    "from keras.layers.core import Dense, Reshape, Lambda,Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Input, Embedding, merge\n",
    "from keras import backend as K\n",
    "from subprocess import check_output\n",
    "#print(check_output(['dir', 'input/']).decode(\"utf8\")) #check the files available in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concatenate all product-ids into a single string\n",
    "# thanks to https://www.kaggle.com/eoakley/start-here-simple-submission\n",
    "\n",
    "def products_concat(series):\n",
    "    out = ''\n",
    "    for product in series:\n",
    "           out = out + str(int(product)) + ' '\n",
    "    \n",
    "    if out != '':\n",
    "        return out.rstrip()\n",
    "    else:\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_split(df):\n",
    "    df_reordered = df[df['reordered'] == 1]\n",
    "    df_reordered = df_reordered.groupby('order_id')['product_id'].apply(products_concat)\n",
    "    \n",
    "    try:\n",
    "        df_reordered = df_reordered.reset_index()\n",
    "        df_reordered.columns = ['order_id','products_list']\n",
    "    except:\n",
    "        df_reordered = df_reordered.reset_index(level = ['order_id'])\n",
    "        df_reordered.columns = ['order_id','products_list']\n",
    "        \n",
    "    return df_reordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "def multilabel_fscore(ytrue, ypred):\n",
    "    \"\"\"\n",
    "    ex1:\n",
    "    y_true = [1, 2, 3]\n",
    "    y_pred = [2, 3]\n",
    "    return: 0.8\n",
    "    \n",
    "    ex2:\n",
    "    y_true = [\"None\"]\n",
    "    y_pred = [2, \"None\"]\n",
    "    return: 0.666\n",
    "    \n",
    "    ex3:\n",
    "    y_true = [4, 5, 6, 7]\n",
    "    y_pred = [2, 4, 8, 9]\n",
    "    return: 0.25\n",
    "    \n",
    "    \"\"\"\n",
    "    #y_true = K.eval(ytrue)\n",
    "    #y_pred = K.eval(ypred)\n",
    "    F1_score = []\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for items in range(len(ytrue)):\n",
    "        y_true = ytrue[items]\n",
    "        y_pred = ypred[items]\n",
    "        correct = sum([1 for i in y_pred if i in y_true])\n",
    "        if correct > 0:\n",
    "\n",
    "            precision = correct / len(y_pred)\n",
    "\n",
    "            recall =    correct / len(y_true)\n",
    "\n",
    "            F1_score.append((2 * precision * recall) / (precision + recall))\n",
    "\n",
    "        else:\n",
    "            F1_score.append(0)\n",
    "\n",
    "    return np.mean(F1_score)\n",
    "\n",
    "#if __name__ != '__main__':\n",
    "\n",
    "print(multilabel_fscore([[2,3],['None']], [[2,3,4],['None']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now let's get and put the data in  pandas dataframe\n",
    "order_products_train = pd.read_csv('input/order_products__train.csv',nrows=100000)\n",
    "order_products_prior = pd.read_csv('input/order_products__prior.csv',nrows=100000)\n",
    "orders = pd.read_csv('input/orders.csv',engine ='c')\n",
    "#aisles = pd.read_csv('input/aisles.csv')\n",
    "#departments = pd.read_csv('input/departments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create one hot encode of aisle and dept\n",
    "products = pd.read_csv('input/products.csv')\n",
    "value = list(products['product_id'].astype(str))\n",
    "value.append('None')\n",
    "df_additional = products[['product_id','aisle_id','department_id']]\n",
    "df_additional[['aisle_id','department_id']] = df_additional[['aisle_id','department_id']].astype(str)\n",
    "df_additional = pd.get_dummies(df_additional, columns=['aisle_id','department_id'])\n",
    "display(df_additional.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_reo = order_products_prior[order_products_prior['reordered'] == 1].drop('reordered', axis=1)\n",
    "display(prior_reo.head(1))\n",
    "prior_reo = prior_reo.merge(df_additional, on='product_id', how='left')\n",
    "print('joined')\n",
    "prior_reo = prior_reo.drop(['product_id','add_to_cart_order'], axis =1)\n",
    "print('aggregating')\n",
    "prior_reo = prior_reo.groupby('order_id').sum()\n",
    "prior_reo = prior_reo.reset_index()\n",
    "#print('saving')\n",
    "#prior_reo.to_csv('input/prior_prods.csv', index=False)\n",
    "display(prior_reo.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reo = order_products_train[order_products_train['reordered'] == 1].drop('reordered', axis=1)\n",
    "display(train_reo.head(1))\n",
    "train_reo = train_reo.merge(df_additional, on='product_id', how='left')\n",
    "train_reo = train_reo.drop(['product_id','add_to_cart_order'], axis =1)\n",
    "print('aggregating')\n",
    "train_reo = train_reo.groupby('order_id').sum()\n",
    "train_reo = train_reo.reset_index()\n",
    "#print('saving')\n",
    "#train_reo.to_csv('input/train_prods.csv', index=False)\n",
    "display(train_reo.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find the number of the last order placed\n",
    "#split orders\n",
    "test_orders  = orders[orders['eval_set'] == 'test' ]\n",
    "prior_orders = orders[orders['eval_set'] == 'prior']\n",
    "train_orders = orders[orders['eval_set'] == 'train']\n",
    "prior_orders['num_orders'] = prior_orders.groupby(['user_id'])['order_number'].transform(max)\n",
    "train_orders['num_orders'] = train_orders.groupby(['user_id'])['order_number'].transform(max)\n",
    "test_orders['num_orders'] = test_orders.groupby(['user_id'])['order_number'].transform(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prior_prod = df_split(order_products_prior)\n",
    "\n",
    "df_train = prior_orders.merge(df_prior_prod, on='order_id', how='left')\n",
    "df_train = df_train.merge(prior_reo, on='order_id', how='left')\n",
    "\n",
    "display(df_train.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_products = df_split(order_products_train)\n",
    "\n",
    "df_eval = train_orders.merge(df_target_products, on='order_id', how='left')\n",
    "df_eval = df_eval.merge(train_reo, on='order_id', how='left')\n",
    "display(df_eval.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['products_list'] = df_train['products_list'].fillna('None')\n",
    "df_eval['products_list'] = df_eval['products_list'].fillna('None')\n",
    "display(df_train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del order_products_train,order_products_prior,orders,products,prior_orders,train_orders\n",
    "del df_target_products,df_prior_prod, train_reo, prior_reo, df_additional\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_eval = df_eval.drop(['eval_set','user_id','order_number'], axis =1)\n",
    "df_train = df_train.drop(['eval_set','user_id','order_number'], axis =1)\n",
    "#display(df_eval[df_eval['products_list']== 'None'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save data\n",
    "pickle.dump(df_eval, open('eval_orders.p', 'wb'), protocol=4)\n",
    "\n",
    "pickle.dump(df_train, open('train_orders.p', 'wb'), protocol=4)\n",
    "\n",
    "pickle.dump(test_orders, open('test_orders.p', 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "data can be loaded from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntest = open('test_orders.p','rb')\\ntest_orders = pickle.load(test)\\ntest.close()\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "evals = open('eval_orders.p','rb')\n",
    "df_eval = pickle.load(evals)\n",
    "evals.close()\n",
    "'''\n",
    "train = open('train_orders.p','rb')\n",
    "df_train = pickle.load(train)\n",
    "train.close()\n",
    "'''\n",
    "test = open('test_orders.p','rb')\n",
    "test_orders = pickle.load(test)\n",
    "test.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['products_list'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_comb = df_train.products_list.unique()\n",
    "display(X_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### setting up keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.set_index('order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['order_dow', 'order_hour_of_day']] = X_train[['order_dow', 'order_hour_of_day']].astype(str)\n",
    "X_train = pd.get_dummies(X_train, columns=['order_dow', 'order_hour_of_day'])\n",
    "\n",
    "X_train = X_train.fillna(0)\n",
    "keras_train = np.array(X_train)\n",
    "\n",
    "display(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vectorise the products list\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "dicts = []\n",
    "for i in value:\n",
    "    dicts.append(tuple(i.split()))\n",
    "print (dicts[49688])\n",
    "\n",
    "\n",
    "vectorizer = MultiLabelBinarizer(sparse_output=True).fit(dicts)\n",
    "\n",
    "#prod_dict = {value:key  for (value, key) in zip(value, key)}\n",
    "#print (list(vectorizer.classes_))\n",
    "#print (key[49688])\n",
    "#print (value[49688])\n",
    "#vectorizer = TfidfVectorizer(vocabulary = prod_dict)\n",
    "#vectorizer = CountVectorizer(vocabulary = prod_dict)\n",
    "train_target = df_train['products_list'].values\n",
    "\n",
    "print(train_target[1])\n",
    "y_train = []\n",
    "for i in train_target:\n",
    "    y_train.append(tuple(i.split()))\n",
    "print (y_train[0])\n",
    "    \n",
    "\n",
    "y_train = vectorizer.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(df_train.iloc[900])\n",
    "\n",
    "#custom round layer\n",
    "from keras.layers import Layer\n",
    "class Round(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Round, self).__init__(**kwargs)\n",
    "\n",
    "    def get_output(self, train=False):\n",
    "        X = self.get_input(train)\n",
    "        return K.round(X)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"name\": self.__class__.__name__}\n",
    "        base_config = super(Round, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "print (vectorizer.classes_[49688]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = y_train[:6000].todense()\n",
    "\n",
    "y_eval = y[5950:]\n",
    "y = y[:5950]\n",
    "\n",
    "test = vectorizer.inverse_transform(y_train[1450:1500])\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#input_layer = Input(shape=(keras_train.shape))\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(66,input_dim=X_train[0].shape, activation='relu'))\n",
    "# Hidden layers\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(124,activation='relu'))\n",
    "model.add(Dense(386,activation='relu'))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dense(700,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "#output layer\n",
    "model.add(Dense(49689, activation='sigmoid'))\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.99, epsilon=1e-03, decay=0.0001)\n",
    "RMSprop = keras.optimizers.RMSprop(lr=0.001)\n",
    "Nadam = keras.optimizers.Nadam(lr=0.001)\n",
    "Adagrad = keras.optimizers.Adagrad(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy',optimizer= RMSprop, metrics = ['categorical_accuracy'])\n",
    "\n",
    "keras_train = np.array(X_train[:5950])\n",
    "keras_eval = np.array(X_train[5950:6000])\n",
    "\n",
    "print (y_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Training')\n",
    "# since we are using stateful rnn tsteps can be set to 1\n",
    "tsteps = 1\n",
    "batch_size = 25\n",
    "epochs = 25\n",
    "# Fit the model\n",
    "\n",
    "model.fit(keras_train, y, epochs=5, batch_size=900,verbose=2, validation_split = 0.05) \n",
    "\n",
    "score = model.evaluate(keras_eval, y_eval)\n",
    "\n",
    "print('\\nTest score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#for i in range(epochs):\n",
    "#    print('Epoch', i, '/', epochs)\n",
    "\n",
    "    # Note that the last state for sample i in a batch will\n",
    "    # be used as initial state for sample i in the next batch.\n",
    "    # Thus we are simultaneously training on batch_size series with\n",
    "    # lower resolution than the original series contained in cos.\n",
    "    # Each of these series are offset by one step and can be\n",
    "    # extracted with cos[i::batch_size].\n",
    "#    model.fit(X_train, train_target,\n",
    "#              batch_size=batch_size,\n",
    "#              epochs=1,\n",
    "#              verbose=1)\n",
    "#    model.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Predicting')\n",
    "predicted_output = model.predict(keras_eval)\n",
    "\n",
    "predicted_output[predicted_output>=0.5] = 1\n",
    "predicted_output[predicted_output<0.5] = 0\n",
    "\n",
    "y_eval_pred = vectorizer.inverse_transform(predicted_output)\n",
    "\n",
    "print('true eval set values {}'.format(test[0]))\n",
    "print('pred eval set values {}\\n'.format(y_eval_pred[0]))\n",
    "\n",
    "print (multilabel_fscore(test,y_eval_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
